{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_time_series_for_window(start_date, end_date, station, dataframe):\n",
    "    return dataframe.loc[start_date:end_date, [station]]\n",
    "\n",
    "\n",
    "def check_nan_values(dataframe):\n",
    "    return dataframe.isnull().values.any()\n",
    "\n",
    "\n",
    "def get_valid_sequences(df):\n",
    "    valid_sequences = []\n",
    "    starting_idx = 0\n",
    "\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        flow = row[0]\n",
    "\n",
    "        if np.isnan(flow):\n",
    "\n",
    "            if starting_idx < i-1:\n",
    "                valid_sequences.append((starting_idx, i))\n",
    "                starting_idx = i+1\n",
    "            else:\n",
    "                starting_idx = i+1\n",
    "                continue\n",
    "    \n",
    "    if not check_nan_values(df.iloc[starting_idx:, :]):\n",
    "        valid_sequences.append((starting_idx, len(df)))\n",
    "    return valid_sequences\n",
    "\n",
    "\n",
    "def valid_seqs_minimum_len(valid_seqs, seq_len):\n",
    "    \n",
    "    valid_seqs_min_len = []\n",
    "    pops = []\n",
    "    for i, (start, end) in enumerate(valid_seqs):\n",
    "        if end - start >= seq_len:\n",
    "            valid_seqs_min_len.append((start, end))\n",
    "\n",
    "    return valid_seqs_min_len\n",
    "\n",
    "\n",
    "\n",
    "def split_sequences(possible_seqs, split_len):\n",
    "    \n",
    "    usable_seqs = []\n",
    "    for seq in possible_seqs:\n",
    "        usable_seqs += get_seq_splits(seq, split_len)\n",
    "        \n",
    "    return usable_seqs\n",
    "        \n",
    "        \n",
    "        \n",
    "def get_seq_splits(seq, split_len):\n",
    "    \n",
    "    start = seq[0]\n",
    "    end = seq[1]\n",
    "    \n",
    "    chunks = (end - start) // (split_len+1)     # +1 because there must be an unobserved item after each chunk\n",
    "                                                # which will be the y (after window value)\n",
    "\n",
    "    splits = []\n",
    "    prev_end_chunk = start\n",
    "    for i in range(chunks):\n",
    "\n",
    "        start_chunk = prev_end_chunk\n",
    "        end_chunk = start_chunk + split_len\n",
    "        splits.append((start_chunk, end_chunk))\n",
    "        prev_end_chunk = end_chunk+1\n",
    "        \n",
    "    return splits\n",
    "\n",
    "\n",
    "def get_seq_obs_values(seq, df):\n",
    "    return np.array(df.iloc[seq[0]:seq[1], :]), np.array(df.iloc[seq[1], :])\n",
    "\n",
    "\n",
    "def split_seqs_train_test(train_frac, usable_seqs):\n",
    "    \n",
    "    total_seqs = len(usable_seqs)\n",
    "    train_amount = round(total_seqs * train_frac)\n",
    "    \n",
    "    random.shuffle(usable_seqs)\n",
    "    train_seqs = usable_seqs[0:train_amount]\n",
    "    test_seqs = usable_seqs[train_amount:]\n",
    "    \n",
    "    return train_seqs, test_seqs\n",
    "\n",
    "\n",
    "def mount_trainable_testable_arrays(seqs, df):\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for seq in seqs:\n",
    "        x, y = get_seq_obs_values(seq, df)\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    \n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n",
    "\n",
    "\n",
    "def transform_cleb_df_into_wal_df(cleb_df):\n",
    "    \n",
    "    index_names = {}\n",
    "    for i, (_, row) in enumerate(cleb_df.iterrows()):\n",
    "        \n",
    "        year = str(int(row[2]))\n",
    "        month = str(int(row[1]))\n",
    "        day = str(int(row[0]))\n",
    "        hour = str(int(row[3]))\n",
    "        index_name = year+'-'+month+'-'+day+'-'+hour\n",
    "        \n",
    "        index_names[i] = index_name\n",
    "        \n",
    "    \n",
    "    cleb_df.rename(index=index_names)\n",
    "    cleb_df = cleb_df.drop('day', 1)\n",
    "    cleb_df = cleb_df.drop('month', 1)\n",
    "    cleb_df = cleb_df.drop('year', 1)\n",
    "    cleb_df = cleb_df.drop('hour', 1)\n",
    "    return cleb_df.replace(-1, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flow_path = \"/home/colombelli/Documents/hydro-ml/data/Vazao.txt\"\n",
    "rain_path = \"/home/colombelli/Documents/hydro-ml/data/Chuva.txt\"\n",
    "et_path = \"/home/colombelli/Documents/hydro-ml/data/ET.txt\"\n",
    "\n",
    "flow_df = pd.read_csv(flow_path, sep=\"\\t\", header=None)\n",
    "flow_df.columns = [\"day\", \"month\", \"year\", \"hour\", \"flow\"]\n",
    "\n",
    "rain_df = pd.read_csv(rain_path, sep=\"\\t\", header=None)\n",
    "rain_df.columns = [\"day\", \"month\", \"year\", \"hour\", \"rain\"]\n",
    "\n",
    "et_df = pd.read_csv(et_path, sep=\"\\t\", header=None)\n",
    "et_df.columns = [\"day\", \"month\", \"year\", \"hour\", \"et\"]\n",
    "\n",
    "\n",
    "flow_df = transform_cleb_df_into_wal_df(flow_df)\n",
    "rain_df = transform_cleb_df_into_wal_df(rain_df)\n",
    "et_df = transform_cleb_df_into_wal_df(et_df)\n",
    "\n",
    "\n",
    "# this fixed val_seq was extracted from the other notebook (no complete automation yet - just a poc)\n",
    "val_seq = [(5962, 6051), (6078, 17486), (17487, 18718), (18741, 23437)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "import os\n",
    "import time\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(rnn_units): \n",
    "      return tf.keras.layers.LSTM(\n",
    "        rnn_units, \n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "        dropout=0.3, \n",
    "        recurrent_dropout=0.3\n",
    "      )\n",
    "\n",
    "\n",
    "### Defining the RNN Model ###\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Layer 1: Embedding layer to transform indices into dense vectors \n",
    "        #   of a fixed embedding size\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "\n",
    "        # Layer 2: LSTM with `rnn_units` number of units. \n",
    "        LSTM(rnn_units),\n",
    "\n",
    "        # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
    "        #   into the vocabulary size.\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model\n",
    "\n",
    "\n",
    "### Defining the loss function ###\n",
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = compute_loss(y, y_hat)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105.84],\n",
       "       [107.3 ],\n",
       "       [108.76],\n",
       "       [109.86],\n",
       "       [110.6 ],\n",
       "       [111.34],\n",
       "       [111.34],\n",
       "       [111.34],\n",
       "       [112.84],\n",
       "       [113.97],\n",
       "       [115.48],\n",
       "       [117.4 ],\n",
       "       [120.11],\n",
       "       [122.86],\n",
       "       [126.05],\n",
       "       [129.7 ],\n",
       "       [134.24],\n",
       "       [138.45],\n",
       "       [142.31],\n",
       "       [145.79],\n",
       "       [150.68],\n",
       "       [155.66],\n",
       "       [159.8 ],\n",
       "       [164.01],\n",
       "       [169.25],\n",
       "       [175.09],\n",
       "       [180.54],\n",
       "       [186.61],\n",
       "       [192.8 ],\n",
       "       [199.12],\n",
       "       [205.02],\n",
       "       [211.02],\n",
       "       [216.57],\n",
       "       [222.22],\n",
       "       [228.52],\n",
       "       [237.34],\n",
       "       [309.1 ],\n",
       "       [480.25],\n",
       "       [603.53],\n",
       "       [619.39],\n",
       "       [632.24],\n",
       "       [646.36],\n",
       "       [659.56],\n",
       "       [675.17],\n",
       "       [687.59],\n",
       "       [702.44],\n",
       "       [716.33],\n",
       "       [729.21],\n",
       "       [744.61],\n",
       "       [756.59],\n",
       "       [772.34],\n",
       "       [785.82],\n",
       "       [798.2 ],\n",
       "       [811.96],\n",
       "       [824.59],\n",
       "       [837.33],\n",
       "       [846.33],\n",
       "       [855.39],\n",
       "       [865.81],\n",
       "       [872.36],\n",
       "       [877.63],\n",
       "       [884.24],\n",
       "       [889.54],\n",
       "       [890.87],\n",
       "       [892.2 ],\n",
       "       [896.2 ],\n",
       "       [896.2 ],\n",
       "       [892.2 ],\n",
       "       [890.87],\n",
       "       [886.89],\n",
       "       [885.56],\n",
       "       [880.26],\n",
       "       [874.99],\n",
       "       [871.05],\n",
       "       [867.12],\n",
       "       [860.59],\n",
       "       [855.39],\n",
       "       [850.21],\n",
       "       [843.75],\n",
       "       [837.33],\n",
       "       [832.22],\n",
       "       [825.85],\n",
       "       [816.99],\n",
       "       [810.7 ],\n",
       "       [801.93],\n",
       "       [794.47],\n",
       "       [785.82],\n",
       "       [777.22],\n",
       "       [768.68],\n",
       "       [758.99],\n",
       "       [748.19],\n",
       "       [737.47],\n",
       "       [728.03],\n",
       "       [715.16],\n",
       "       [702.44],\n",
       "       [692.13],\n",
       "       [677.42],\n",
       "       [664.  ],\n",
       "       [650.74],\n",
       "       [633.33],\n",
       "       [622.58],\n",
       "       [608.78],\n",
       "       [595.19],\n",
       "       [582.79],\n",
       "       [569.54],\n",
       "       [559.48],\n",
       "       [556.48],\n",
       "       [556.48],\n",
       "       [556.48],\n",
       "       [556.48],\n",
       "       [555.49],\n",
       "       [552.5 ],\n",
       "       [550.52],\n",
       "       [551.51],\n",
       "       [552.5 ],\n",
       "       [552.5 ],\n",
       "       [552.5 ],\n",
       "       [548.53],\n",
       "       [548.53],\n",
       "       [548.53],\n",
       "       [548.53],\n",
       "       [545.58],\n",
       "       [544.59],\n",
       "       [544.59],\n",
       "       [544.59],\n",
       "       [542.63],\n",
       "       [540.66],\n",
       "       [540.66],\n",
       "       [540.66],\n",
       "       [540.66],\n",
       "       [537.73],\n",
       "       [536.75],\n",
       "       [536.75],\n",
       "       [536.75],\n",
       "       [536.75],\n",
       "       [532.85],\n",
       "       [532.85],\n",
       "       [532.85],\n",
       "       [532.85],\n",
       "       [532.85],\n",
       "       [529.95],\n",
       "       [528.98],\n",
       "       [528.98],\n",
       "       [528.98],\n",
       "       [527.05],\n",
       "       [525.12],\n",
       "       [525.12],\n",
       "       [525.12],\n",
       "       [525.12],\n",
       "       [523.2 ],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [519.37],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [516.5 ],\n",
       "       [517.45],\n",
       "       [515.55],\n",
       "       [516.5 ],\n",
       "       [516.5 ],\n",
       "       [515.55],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [514.59],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [518.41],\n",
       "       [520.32],\n",
       "       [520.32],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [521.28],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [517.45],\n",
       "       [515.55],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [513.64],\n",
       "       [512.69],\n",
       "       [509.85],\n",
       "       [509.85],\n",
       "       [509.85],\n",
       "       [509.85],\n",
       "       [509.85],\n",
       "       [509.85],\n",
       "       [507.02],\n",
       "       [506.08],\n",
       "       [506.08],\n",
       "       [506.08],\n",
       "       [506.08],\n",
       "       [506.08],\n",
       "       [505.14],\n",
       "       [502.33],\n",
       "       [502.33],\n",
       "       [502.33],\n",
       "       [502.33],\n",
       "       [502.33],\n",
       "       [499.53],\n",
       "       [498.59],\n",
       "       [498.59],\n",
       "       [498.59],\n",
       "       [498.59],\n",
       "       [495.8 ],\n",
       "       [494.87],\n",
       "       [494.87],\n",
       "       [494.87],\n",
       "       [494.87],\n",
       "       [492.09],\n",
       "       [491.16],\n",
       "       [491.16],\n",
       "       [491.16],\n",
       "       [490.24],\n",
       "       [487.47],\n",
       "       [487.47],\n",
       "       [487.47],\n",
       "       [487.47],\n",
       "       [485.64],\n",
       "       [483.8 ],\n",
       "       [483.8 ],\n",
       "       [483.8 ],\n",
       "       [483.8 ],\n",
       "       [483.8 ],\n",
       "       [483.8 ],\n",
       "       [480.15],\n",
       "       [480.15],\n",
       "       [480.15],\n",
       "       [478.33],\n",
       "       [476.51],\n",
       "       [476.51],\n",
       "       [476.51],\n",
       "       [472.89],\n",
       "       [472.89],\n",
       "       [472.89],\n",
       "       [471.09],\n",
       "       [469.29],\n",
       "       [469.29],\n",
       "       [466.61],\n",
       "       [465.71],\n",
       "       [465.71],\n",
       "       [462.14],\n",
       "       [462.14],\n",
       "       [462.14],\n",
       "       [460.36],\n",
       "       [458.58],\n",
       "       [458.58],\n",
       "       [455.93],\n",
       "       [455.05],\n",
       "       [455.05],\n",
       "       [452.41],\n",
       "       [451.53],\n",
       "       [451.53],\n",
       "       [448.03],\n",
       "       [448.03],\n",
       "       [448.03],\n",
       "       [445.41],\n",
       "       [444.54],\n",
       "       [442.81],\n",
       "       [441.08],\n",
       "       [441.08],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [434.19],\n",
       "       [434.19],\n",
       "       [433.34],\n",
       "       [430.77],\n",
       "       [430.77],\n",
       "       [427.37],\n",
       "       [427.37],\n",
       "       [426.52],\n",
       "       [423.98],\n",
       "       [423.98],\n",
       "       [423.14],\n",
       "       [420.61],\n",
       "       [420.61],\n",
       "       [418.94],\n",
       "       [418.1 ],\n",
       "       [417.26],\n",
       "       [417.26],\n",
       "       [415.59],\n",
       "       [413.92],\n",
       "       [413.92],\n",
       "       [413.92],\n",
       "       [411.43],\n",
       "       [410.6 ],\n",
       "       [410.6 ],\n",
       "       [410.6 ],\n",
       "       [408.13],\n",
       "       [407.3 ],\n",
       "       [407.3 ],\n",
       "       [407.3 ],\n",
       "       [404.83],\n",
       "       [404.83],\n",
       "       [404.83],\n",
       "       [404.01],\n",
       "       [404.83],\n",
       "       [404.01],\n",
       "       [404.83],\n",
       "       [406.48],\n",
       "       [406.48],\n",
       "       [407.3 ],\n",
       "       [408.13],\n",
       "       [410.6 ],\n",
       "       [413.92],\n",
       "       [416.43],\n",
       "       [419.77],\n",
       "       [423.14],\n",
       "       [426.52],\n",
       "       [429.07],\n",
       "       [431.63],\n",
       "       [434.19],\n",
       "       [436.76],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [437.62],\n",
       "       [435.05],\n",
       "       [434.19],\n",
       "       [431.63],\n",
       "       [430.77],\n",
       "       [427.37],\n",
       "       [427.37],\n",
       "       [423.98],\n",
       "       [423.14],\n",
       "       [420.61],\n",
       "       [419.77],\n",
       "       [417.26],\n",
       "       [416.43],\n",
       "       [413.92],\n",
       "       [413.92],\n",
       "       [411.43],\n",
       "       [410.6 ],\n",
       "       [408.95],\n",
       "       [407.3 ],\n",
       "       [407.3 ],\n",
       "       [406.48],\n",
       "       [404.01],\n",
       "       [404.01],\n",
       "       [403.19],\n",
       "       [400.74],\n",
       "       [400.74],\n",
       "       [399.93],\n",
       "       [397.49],\n",
       "       [397.49],\n",
       "       [395.87],\n",
       "       [394.25],\n",
       "       [394.25],\n",
       "       [393.45],\n",
       "       [391.03],\n",
       "       [391.03],\n",
       "       [389.43],\n",
       "       [387.82],\n",
       "       [387.82],\n",
       "       [386.23],\n",
       "       [384.63],\n",
       "       [384.63],\n",
       "       [384.63],\n",
       "       [382.25],\n",
       "       [381.46],\n",
       "       [379.09],\n",
       "       [378.3 ],\n",
       "       [378.3 ],\n",
       "       [378.3 ],\n",
       "       [375.16],\n",
       "       [375.16],\n",
       "       [368.92],\n",
       "       [365.83],\n",
       "       [364.29],\n",
       "       [362.75],\n",
       "       [360.46],\n",
       "       [358.93],\n",
       "       [356.64],\n",
       "       [355.13],\n",
       "       [353.61],\n",
       "       [350.6 ],\n",
       "       [349.1 ],\n",
       "       [347.6 ],\n",
       "       [346.86],\n",
       "       [346.86],\n",
       "       [346.11],\n",
       "       [344.62],\n",
       "       [342.39],\n",
       "       [342.4 ],\n",
       "       [340.18],\n",
       "       [339.44],\n",
       "       [338.7 ],\n",
       "       [338.7 ],\n",
       "       [338.7 ],\n",
       "       [333.58],\n",
       "       [331.39],\n",
       "       [329.94],\n",
       "       [326.33],\n",
       "       [324.9 ],\n",
       "       [322.04],\n",
       "       [321.32],\n",
       "       [320.61],\n",
       "       [318.48],\n",
       "       [318.48],\n",
       "       [318.48],\n",
       "       [315.65],\n",
       "       [314.25],\n",
       "       [311.44],\n",
       "       [310.04],\n",
       "       [310.04],\n",
       "       [307.96],\n",
       "       [306.57],\n",
       "       [304.5 ],\n",
       "       [304.5 ],\n",
       "       [303.13],\n",
       "       [301.75],\n",
       "       [299.7 ],\n",
       "       [299.01],\n",
       "       [298.33],\n",
       "       [296.29],\n",
       "       [296.29],\n",
       "       [294.94],\n",
       "       [293.59],\n",
       "       [291.57],\n",
       "       [289.57],\n",
       "       [288.23],\n",
       "       [287.57],\n",
       "       [285.57],\n",
       "       [282.92],\n",
       "       [282.92],\n",
       "       [281.61],\n",
       "       [280.3 ],\n",
       "       [278.99],\n",
       "       [280.3 ],\n",
       "       [278.34],\n",
       "       [278.34],\n",
       "       [276.38],\n",
       "       [275.08],\n",
       "       [273.79],\n",
       "       [272.5 ],\n",
       "       [270.57],\n",
       "       [269.93],\n",
       "       [269.93],\n",
       "       [267.38],\n",
       "       [266.75],\n",
       "       [264.84],\n",
       "       [262.94],\n",
       "       [262.31],\n",
       "       [261.06],\n",
       "       [260.43],\n",
       "       [259.8 ],\n",
       "       [258.56],\n",
       "       [256.69],\n",
       "       [254.83],\n",
       "       [253.6 ],\n",
       "       [252.36],\n",
       "       [249.91],\n",
       "       [249.91],\n",
       "       [249.91],\n",
       "       [249.91],\n",
       "       [248.7 ],\n",
       "       [247.48],\n",
       "       [245.67],\n",
       "       [245.06],\n",
       "       [242.65],\n",
       "       [242.65],\n",
       "       [242.05],\n",
       "       [240.26],\n",
       "       [239.67],\n",
       "       [237.88],\n",
       "       [237.88],\n",
       "       [235.52],\n",
       "       [235.52],\n",
       "       [233.17],\n",
       "       [233.17],\n",
       "       [232.  ],\n",
       "       [230.83],\n",
       "       [230.83],\n",
       "       [230.83],\n",
       "       [230.25],\n",
       "       [229.09],\n",
       "       [228.51],\n",
       "       [226.21],\n",
       "       [226.21],\n",
       "       [226.21],\n",
       "       [226.21],\n",
       "       [226.21],\n",
       "       [225.07],\n",
       "       [223.92],\n",
       "       [223.92],\n",
       "       [221.64],\n",
       "       [221.64],\n",
       "       [219.38],\n",
       "       [219.38],\n",
       "       [219.38],\n",
       "       [218.82],\n",
       "       [217.69],\n",
       "       [217.13],\n",
       "       [214.89],\n",
       "       [214.89],\n",
       "       [212.67],\n",
       "       [212.67],\n",
       "       [211.02],\n",
       "       [210.47],\n",
       "       [209.37],\n",
       "       [208.27],\n",
       "       [208.27],\n",
       "       [209.92],\n",
       "       [208.27],\n",
       "       [207.19],\n",
       "       [205.56],\n",
       "       [203.93],\n",
       "       [205.02],\n",
       "       [203.93],\n",
       "       [202.86],\n",
       "       [201.78],\n",
       "       [201.78],\n",
       "       [200.71],\n",
       "       [200.71],\n",
       "       [199.64],\n",
       "       [199.64],\n",
       "       [199.64],\n",
       "       [198.05],\n",
       "       [196.47],\n",
       "       [195.41],\n",
       "       [195.41],\n",
       "       [194.89],\n",
       "       [193.84],\n",
       "       [193.32],\n",
       "       [193.32],\n",
       "       [191.24],\n",
       "       [191.24],\n",
       "       [191.24],\n",
       "       [191.24],\n",
       "       [189.17],\n",
       "       [187.12],\n",
       "       [187.12],\n",
       "       [187.12],\n",
       "       [186.61],\n",
       "       [185.08],\n",
       "       [183.05],\n",
       "       [182.55],\n",
       "       [182.05],\n",
       "       [181.04],\n",
       "       [181.04],\n",
       "       [180.54],\n",
       "       [179.04],\n",
       "       [178.05],\n",
       "       [178.55],\n",
       "       [177.06],\n",
       "       [177.06],\n",
       "       [177.06],\n",
       "       [177.06],\n",
       "       [175.09],\n",
       "       [175.09],\n",
       "       [174.6 ],\n",
       "       [173.13],\n",
       "       [173.13],\n",
       "       [171.18],\n",
       "       [171.18],\n",
       "       [169.25],\n",
       "       [169.25],\n",
       "       [169.25],\n",
       "       [165.91],\n",
       "       [164.96],\n",
       "       [165.43],\n",
       "       [164.49],\n",
       "       [163.54],\n",
       "       [163.54],\n",
       "       [163.07],\n",
       "       [161.66],\n",
       "       [161.66],\n",
       "       [161.66],\n",
       "       [160.73],\n",
       "       [159.8 ],\n",
       "       [159.8 ],\n",
       "       [159.8 ],\n",
       "       [159.8 ],\n",
       "       [158.88],\n",
       "       [157.95],\n",
       "       [157.95],\n",
       "       [156.11],\n",
       "       [156.11],\n",
       "       [155.65],\n",
       "       [154.28],\n",
       "       [153.83],\n",
       "       [152.47],\n",
       "       [152.47],\n",
       "       [152.47],\n",
       "       [151.12],\n",
       "       [150.67],\n",
       "       [150.23],\n",
       "       [149.78],\n",
       "       [149.34],\n",
       "       [148.89],\n",
       "       [148.89],\n",
       "       [148.  ],\n",
       "       [147.11],\n",
       "       [147.11],\n",
       "       [147.11],\n",
       "       [146.23],\n",
       "       [145.35],\n",
       "       [145.35],\n",
       "       [143.61],\n",
       "       [143.61],\n",
       "       [143.61],\n",
       "       [142.31],\n",
       "       [141.87],\n",
       "       [141.87],\n",
       "       [141.44],\n",
       "       [140.15],\n",
       "       [140.15],\n",
       "       [139.3 ],\n",
       "       [138.45],\n",
       "       [138.45],\n",
       "       [137.18],\n",
       "       [136.75],\n",
       "       [136.75],\n",
       "       [136.75],\n",
       "       [136.33],\n",
       "       [135.07],\n",
       "       [135.07],\n",
       "       [135.07],\n",
       "       [135.07],\n",
       "       [133.82],\n",
       "       [133.4 ],\n",
       "       [133.4 ],\n",
       "       [133.4 ],\n",
       "       [132.99],\n",
       "       [131.74],\n",
       "       [131.74],\n",
       "       [131.74],\n",
       "       [131.74],\n",
       "       [130.1 ],\n",
       "       [130.1 ],\n",
       "       [130.1 ],\n",
       "       [130.1 ],\n",
       "       [130.1 ],\n",
       "       [129.29],\n",
       "       [128.47],\n",
       "       [128.47],\n",
       "       [128.07],\n",
       "       [126.85],\n",
       "       [126.85],\n",
       "       [126.45],\n",
       "       [126.85],\n",
       "       [125.24],\n",
       "       [125.24],\n",
       "       [125.24],\n",
       "       [124.84],\n",
       "       [123.65],\n",
       "       [123.65],\n",
       "       [122.06],\n",
       "       [122.06],\n",
       "       [122.06],\n",
       "       [121.67],\n",
       "       [120.5 ],\n",
       "       [120.5 ],\n",
       "       [120.11],\n",
       "       [118.94],\n",
       "       [118.94],\n",
       "       [118.94],\n",
       "       [117.78],\n",
       "       [117.39],\n",
       "       [117.39],\n",
       "       [117.39],\n",
       "       [117.39],\n",
       "       [117.39],\n",
       "       [116.24],\n",
       "       [115.86],\n",
       "       [115.86]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 30*24\n",
    "np.array(flow_df.iloc[7000:7001+window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105.84,\n",
       " 107.3,\n",
       " 108.76,\n",
       " 109.86,\n",
       " 110.6,\n",
       " 111.34,\n",
       " 111.34,\n",
       " 111.34,\n",
       " 112.84,\n",
       " 113.97,\n",
       " 115.48,\n",
       " 117.4,\n",
       " 120.11,\n",
       " 122.86,\n",
       " 126.05,\n",
       " 129.7,\n",
       " 134.24,\n",
       " 138.45,\n",
       " 142.31,\n",
       " 145.79,\n",
       " 150.68,\n",
       " 155.66,\n",
       " 159.8,\n",
       " 164.01,\n",
       " 169.25,\n",
       " 175.09,\n",
       " 180.54,\n",
       " 186.61,\n",
       " 192.8,\n",
       " 199.12,\n",
       " 205.02,\n",
       " 211.02,\n",
       " 216.57,\n",
       " 222.22,\n",
       " 228.52,\n",
       " 237.34,\n",
       " 309.1,\n",
       " 480.25,\n",
       " 603.53,\n",
       " 619.39,\n",
       " 632.24,\n",
       " 646.36,\n",
       " 659.56,\n",
       " 675.17,\n",
       " 687.59,\n",
       " 702.44,\n",
       " 716.33,\n",
       " 729.21,\n",
       " 744.61,\n",
       " 756.59,\n",
       " 772.34,\n",
       " 785.82,\n",
       " 798.2,\n",
       " 811.96,\n",
       " 824.59,\n",
       " 837.33,\n",
       " 846.33,\n",
       " 855.39,\n",
       " 865.81,\n",
       " 872.36,\n",
       " 877.63,\n",
       " 884.24,\n",
       " 889.54,\n",
       " 890.87,\n",
       " 892.2,\n",
       " 896.2,\n",
       " 896.2,\n",
       " 892.2,\n",
       " 890.87,\n",
       " 886.89,\n",
       " 885.56,\n",
       " 880.26,\n",
       " 874.99,\n",
       " 871.05,\n",
       " 867.12,\n",
       " 860.59,\n",
       " 855.39,\n",
       " 850.21,\n",
       " 843.75,\n",
       " 837.33,\n",
       " 832.22,\n",
       " 825.85,\n",
       " 816.99,\n",
       " 810.7,\n",
       " 801.93,\n",
       " 794.47,\n",
       " 785.82,\n",
       " 777.22,\n",
       " 768.68,\n",
       " 758.99,\n",
       " 748.19,\n",
       " 737.47,\n",
       " 728.03,\n",
       " 715.16,\n",
       " 702.44,\n",
       " 692.13,\n",
       " 677.42,\n",
       " 664.0,\n",
       " 650.74,\n",
       " 633.33,\n",
       " 622.58,\n",
       " 608.78,\n",
       " 595.19,\n",
       " 582.79,\n",
       " 569.54,\n",
       " 559.48,\n",
       " 556.48,\n",
       " 556.48,\n",
       " 556.48,\n",
       " 556.48,\n",
       " 555.49,\n",
       " 552.5,\n",
       " 550.52,\n",
       " 551.51,\n",
       " 552.5,\n",
       " 552.5,\n",
       " 552.5,\n",
       " 548.53,\n",
       " 548.53,\n",
       " 548.53,\n",
       " 548.53,\n",
       " 545.58,\n",
       " 544.59,\n",
       " 544.59,\n",
       " 544.59,\n",
       " 542.63,\n",
       " 540.66,\n",
       " 540.66,\n",
       " 540.66,\n",
       " 540.66,\n",
       " 537.73,\n",
       " 536.75,\n",
       " 536.75,\n",
       " 536.75,\n",
       " 536.75,\n",
       " 532.85,\n",
       " 532.85,\n",
       " 532.85,\n",
       " 532.85,\n",
       " 532.85,\n",
       " 529.95,\n",
       " 528.98,\n",
       " 528.98,\n",
       " 528.98,\n",
       " 527.05,\n",
       " 525.12,\n",
       " 525.12,\n",
       " 525.12,\n",
       " 525.12,\n",
       " 523.2,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 519.37,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 516.5,\n",
       " 517.45,\n",
       " 515.55,\n",
       " 516.5,\n",
       " 516.5,\n",
       " 515.55,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 514.59,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 518.41,\n",
       " 520.32,\n",
       " 520.32,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 521.28,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 517.45,\n",
       " 515.55,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 513.64,\n",
       " 512.69,\n",
       " 509.85,\n",
       " 509.85,\n",
       " 509.85,\n",
       " 509.85,\n",
       " 509.85,\n",
       " 509.85,\n",
       " 507.02,\n",
       " 506.08,\n",
       " 506.08,\n",
       " 506.08,\n",
       " 506.08,\n",
       " 506.08,\n",
       " 505.14,\n",
       " 502.33,\n",
       " 502.33,\n",
       " 502.33,\n",
       " 502.33,\n",
       " 502.33,\n",
       " 499.53,\n",
       " 498.59,\n",
       " 498.59,\n",
       " 498.59,\n",
       " 498.59,\n",
       " 495.8,\n",
       " 494.87,\n",
       " 494.87,\n",
       " 494.87,\n",
       " 494.87,\n",
       " 492.09,\n",
       " 491.16,\n",
       " 491.16,\n",
       " 491.16,\n",
       " 490.24,\n",
       " 487.47,\n",
       " 487.47,\n",
       " 487.47,\n",
       " 487.47,\n",
       " 485.64,\n",
       " 483.8,\n",
       " 483.8,\n",
       " 483.8,\n",
       " 483.8,\n",
       " 483.8,\n",
       " 483.8,\n",
       " 480.15,\n",
       " 480.15,\n",
       " 480.15,\n",
       " 478.33,\n",
       " 476.51,\n",
       " 476.51,\n",
       " 476.51,\n",
       " 472.89,\n",
       " 472.89,\n",
       " 472.89,\n",
       " 471.09,\n",
       " 469.29,\n",
       " 469.29,\n",
       " 466.61,\n",
       " 465.71,\n",
       " 465.71,\n",
       " 462.14,\n",
       " 462.14,\n",
       " 462.14,\n",
       " 460.36,\n",
       " 458.58,\n",
       " 458.58,\n",
       " 455.93,\n",
       " 455.05,\n",
       " 455.05,\n",
       " 452.41,\n",
       " 451.53,\n",
       " 451.53,\n",
       " 448.03,\n",
       " 448.03,\n",
       " 448.03,\n",
       " 445.41,\n",
       " 444.54,\n",
       " 442.81,\n",
       " 441.08,\n",
       " 441.08,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 434.19,\n",
       " 434.19,\n",
       " 433.34,\n",
       " 430.77,\n",
       " 430.77,\n",
       " 427.37,\n",
       " 427.37,\n",
       " 426.52,\n",
       " 423.98,\n",
       " 423.98,\n",
       " 423.14,\n",
       " 420.61,\n",
       " 420.61,\n",
       " 418.94,\n",
       " 418.1,\n",
       " 417.26,\n",
       " 417.26,\n",
       " 415.59,\n",
       " 413.92,\n",
       " 413.92,\n",
       " 413.92,\n",
       " 411.43,\n",
       " 410.6,\n",
       " 410.6,\n",
       " 410.6,\n",
       " 408.13,\n",
       " 407.3,\n",
       " 407.3,\n",
       " 407.3,\n",
       " 404.83,\n",
       " 404.83,\n",
       " 404.83,\n",
       " 404.01,\n",
       " 404.83,\n",
       " 404.01,\n",
       " 404.83,\n",
       " 406.48,\n",
       " 406.48,\n",
       " 407.3,\n",
       " 408.13,\n",
       " 410.6,\n",
       " 413.92,\n",
       " 416.43,\n",
       " 419.77,\n",
       " 423.14,\n",
       " 426.52,\n",
       " 429.07,\n",
       " 431.63,\n",
       " 434.19,\n",
       " 436.76,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 437.62,\n",
       " 435.05,\n",
       " 434.19,\n",
       " 431.63,\n",
       " 430.77,\n",
       " 427.37,\n",
       " 427.37,\n",
       " 423.98,\n",
       " 423.14,\n",
       " 420.61,\n",
       " 419.77,\n",
       " 417.26,\n",
       " 416.43,\n",
       " 413.92,\n",
       " 413.92,\n",
       " 411.43,\n",
       " 410.6,\n",
       " 408.95,\n",
       " 407.3,\n",
       " 407.3,\n",
       " 406.48,\n",
       " 404.01,\n",
       " 404.01,\n",
       " 403.19,\n",
       " 400.74,\n",
       " 400.74,\n",
       " 399.93,\n",
       " 397.49,\n",
       " 397.49,\n",
       " 395.87,\n",
       " 394.25,\n",
       " 394.25,\n",
       " 393.45,\n",
       " 391.03,\n",
       " 391.03,\n",
       " 389.43,\n",
       " 387.82,\n",
       " 387.82,\n",
       " 386.23,\n",
       " 384.63,\n",
       " 384.63,\n",
       " 384.63,\n",
       " 382.25,\n",
       " 381.46,\n",
       " 379.09,\n",
       " 378.3,\n",
       " 378.3,\n",
       " 378.3,\n",
       " 375.16,\n",
       " 375.16,\n",
       " 368.92,\n",
       " 365.83,\n",
       " 364.29,\n",
       " 362.75,\n",
       " 360.46,\n",
       " 358.93,\n",
       " 356.64,\n",
       " 355.13,\n",
       " 353.61,\n",
       " 350.6,\n",
       " 349.1,\n",
       " 347.6,\n",
       " 346.86,\n",
       " 346.86,\n",
       " 346.11,\n",
       " 344.62,\n",
       " 342.39,\n",
       " 342.4,\n",
       " 340.18,\n",
       " 339.44,\n",
       " 338.7,\n",
       " 338.7,\n",
       " 338.7,\n",
       " 333.58,\n",
       " 331.39,\n",
       " 329.94,\n",
       " 326.33,\n",
       " 324.9,\n",
       " 322.04,\n",
       " 321.32,\n",
       " 320.61,\n",
       " 318.48,\n",
       " 318.48,\n",
       " 318.48,\n",
       " 315.65,\n",
       " 314.25,\n",
       " 311.44,\n",
       " 310.04,\n",
       " 310.04,\n",
       " 307.96,\n",
       " 306.57,\n",
       " 304.5,\n",
       " 304.5,\n",
       " 303.13,\n",
       " 301.75,\n",
       " 299.7,\n",
       " 299.01,\n",
       " 298.33,\n",
       " 296.29,\n",
       " 296.29,\n",
       " 294.94,\n",
       " 293.59,\n",
       " 291.57,\n",
       " 289.57,\n",
       " 288.23,\n",
       " 287.57,\n",
       " 285.57,\n",
       " 282.92,\n",
       " 282.92,\n",
       " 281.61,\n",
       " 280.3,\n",
       " 278.99,\n",
       " 280.3,\n",
       " 278.34,\n",
       " 278.34,\n",
       " 276.38,\n",
       " 275.08,\n",
       " 273.79,\n",
       " 272.5,\n",
       " 270.57,\n",
       " 269.93,\n",
       " 269.93,\n",
       " 267.38,\n",
       " 266.75,\n",
       " 264.84,\n",
       " 262.94,\n",
       " 262.31,\n",
       " 261.06,\n",
       " 260.43,\n",
       " 259.8,\n",
       " 258.56,\n",
       " 256.69,\n",
       " 254.83,\n",
       " 253.6,\n",
       " 252.36,\n",
       " 249.91,\n",
       " 249.91,\n",
       " 249.91,\n",
       " 249.91,\n",
       " 248.7,\n",
       " 247.48,\n",
       " 245.67,\n",
       " 245.06,\n",
       " 242.65,\n",
       " 242.65,\n",
       " 242.05,\n",
       " 240.26,\n",
       " 239.67,\n",
       " 237.88,\n",
       " 237.88,\n",
       " 235.52,\n",
       " 235.52,\n",
       " 233.17,\n",
       " 233.17,\n",
       " 232.0,\n",
       " 230.83,\n",
       " 230.83,\n",
       " 230.83,\n",
       " 230.25,\n",
       " 229.09,\n",
       " 228.51,\n",
       " 226.21,\n",
       " 226.21,\n",
       " 226.21,\n",
       " 226.21,\n",
       " 226.21,\n",
       " 225.07,\n",
       " 223.92,\n",
       " 223.92,\n",
       " 221.64,\n",
       " 221.64,\n",
       " 219.38,\n",
       " 219.38,\n",
       " 219.38,\n",
       " 218.82,\n",
       " 217.69,\n",
       " 217.13,\n",
       " 214.89,\n",
       " 214.89,\n",
       " 212.67,\n",
       " 212.67,\n",
       " 211.02,\n",
       " 210.47,\n",
       " 209.37,\n",
       " 208.27,\n",
       " 208.27,\n",
       " 209.92,\n",
       " 208.27,\n",
       " 207.19,\n",
       " 205.56,\n",
       " 203.93,\n",
       " 205.02,\n",
       " 203.93,\n",
       " 202.86,\n",
       " 201.78,\n",
       " 201.78,\n",
       " 200.71,\n",
       " 200.71,\n",
       " 199.64,\n",
       " 199.64,\n",
       " 199.64,\n",
       " 198.05,\n",
       " 196.47,\n",
       " 195.41,\n",
       " 195.41,\n",
       " 194.89,\n",
       " 193.84,\n",
       " 193.32,\n",
       " 193.32,\n",
       " 191.24,\n",
       " 191.24,\n",
       " 191.24,\n",
       " 191.24,\n",
       " 189.17,\n",
       " 187.12,\n",
       " 187.12,\n",
       " 187.12,\n",
       " 186.61,\n",
       " 185.08,\n",
       " 183.05,\n",
       " 182.55,\n",
       " 182.05,\n",
       " 181.04,\n",
       " 181.04,\n",
       " 180.54,\n",
       " 179.04,\n",
       " 178.05,\n",
       " 178.55,\n",
       " 177.06,\n",
       " 177.06,\n",
       " 177.06,\n",
       " 177.06,\n",
       " 175.09,\n",
       " 175.09,\n",
       " 174.6,\n",
       " 173.13,\n",
       " 173.13,\n",
       " 171.18,\n",
       " 171.18,\n",
       " 169.25,\n",
       " 169.25,\n",
       " 169.25,\n",
       " 165.91,\n",
       " 164.96,\n",
       " 165.43,\n",
       " 164.49,\n",
       " 163.54,\n",
       " 163.54,\n",
       " 163.07,\n",
       " 161.66,\n",
       " 161.66,\n",
       " 161.66,\n",
       " 160.73,\n",
       " 159.8,\n",
       " 159.8,\n",
       " 159.8,\n",
       " 159.8,\n",
       " 158.88,\n",
       " 157.95,\n",
       " 157.95,\n",
       " 156.11,\n",
       " 156.11,\n",
       " 155.65,\n",
       " 154.28,\n",
       " 153.83,\n",
       " 152.47,\n",
       " 152.47,\n",
       " 152.47,\n",
       " 151.12,\n",
       " 150.67,\n",
       " 150.23,\n",
       " 149.78,\n",
       " 149.34,\n",
       " 148.89,\n",
       " 148.89,\n",
       " 148.0,\n",
       " 147.11,\n",
       " 147.11,\n",
       " 147.11,\n",
       " 146.23,\n",
       " 145.35,\n",
       " 145.35,\n",
       " 143.61,\n",
       " 143.61,\n",
       " 143.61,\n",
       " 142.31,\n",
       " 141.87,\n",
       " 141.87,\n",
       " 141.44,\n",
       " 140.15,\n",
       " 140.15,\n",
       " 139.3,\n",
       " 138.45,\n",
       " 138.45,\n",
       " 137.18,\n",
       " 136.75,\n",
       " 136.75,\n",
       " 136.75,\n",
       " 136.33,\n",
       " 135.07,\n",
       " 135.07,\n",
       " 135.07,\n",
       " 135.07,\n",
       " 133.82,\n",
       " 133.4,\n",
       " 133.4,\n",
       " 133.4,\n",
       " 132.99,\n",
       " 131.74,\n",
       " 131.74,\n",
       " 131.74,\n",
       " 131.74,\n",
       " 130.1,\n",
       " 130.1,\n",
       " 130.1,\n",
       " 130.1,\n",
       " 130.1,\n",
       " 129.29,\n",
       " 128.47,\n",
       " 128.47,\n",
       " 128.07,\n",
       " 126.85,\n",
       " 126.85,\n",
       " 126.45,\n",
       " 126.85,\n",
       " 125.24,\n",
       " 125.24,\n",
       " 125.24,\n",
       " 124.84,\n",
       " 123.65,\n",
       " 123.65,\n",
       " 122.06,\n",
       " 122.06,\n",
       " 122.06,\n",
       " 121.67,\n",
       " 120.5,\n",
       " 120.5,\n",
       " 120.11,\n",
       " 118.94,\n",
       " 118.94,\n",
       " 118.94,\n",
       " 117.78,\n",
       " 117.39,\n",
       " 117.39,\n",
       " 117.39,\n",
       " 117.39,\n",
       " 117.39,\n",
       " 116.24,\n",
       " 115.86,\n",
       " 115.86]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.squeeze(np.array(flow_df.iloc[7000:7001+window])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 2) (5, 2)\n",
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] [ 85 105]\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] [105 125]\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] [125 145]\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] [145 165]\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] [165 185]\n"
     ]
    }
   ],
   "source": [
    "# multivariate multi-step data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "print(X.shape, y.shape)\n",
    "print(dataset)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([flow_df, rain_df, et_df, flow_df], axis=1)\n",
    "dataset.columns = ['flow', 'rain', 'et', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow</th>\n",
       "      <th>rain</th>\n",
       "      <th>et</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.80</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>159.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158.41</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>158.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>157.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.11</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>156.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.28</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>154.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25949</th>\n",
       "      <td>183.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>183.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25950</th>\n",
       "      <td>181.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>181.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25951</th>\n",
       "      <td>181.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>181.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25952</th>\n",
       "      <td>181.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>181.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25953</th>\n",
       "      <td>180.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>180.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25954 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         flow  rain      et       y\n",
       "0      159.80   3.6  0.0555  159.80\n",
       "1      158.41   2.6  0.0555  158.41\n",
       "2      157.49   1.0  0.0555  157.49\n",
       "3      156.11   1.2  0.0555  156.11\n",
       "4      154.28   5.6  0.0555  154.28\n",
       "...       ...   ...     ...     ...\n",
       "25949  183.05   0.0  0.1480  183.05\n",
       "25950  181.04   0.0  0.1480  181.04\n",
       "25951  181.04   0.0  0.1480  181.04\n",
       "25952  181.04   0.0  0.1480  181.04\n",
       "25953  180.54   0.0  0.1480  180.54\n",
       "\n",
       "[25954 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window without NA (17487, 18718) after that i could try (6078, 17486)\n",
    "to_split = np.array(dataset.iloc[17487:18718, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 24*30   #24 hours, 30 days\n",
    "n_steps_out = 24*7   #24 hours, 7 days\n",
    "\n",
    "X, y = split_sequences(to_split, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='sigmoid', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(128, activation='sigmoid'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 8s 745ms/step - loss: 1917.8132\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 8s 757ms/step - loss: 1811.3824\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 8s 760ms/step - loss: 1673.3735\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 9s 814ms/step - loss: 1534.2856\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 9s 825ms/step - loss: 1410.4071\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 9s 835ms/step - loss: 1294.5298\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 9s 824ms/step - loss: 1188.9178\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 9s 817ms/step - loss: 1091.3947\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 9s 834ms/step - loss: 1002.6118\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 9s 816ms/step - loss: 916.4674\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 9s 826ms/step - loss: 839.6967\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 9s 810ms/step - loss: 769.9505\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 9s 824ms/step - loss: 704.7914\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 9s 816ms/step - loss: 645.1787\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 9s 825ms/step - loss: 590.4009\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 9s 811ms/step - loss: 539.7908\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 9s 809ms/step - loss: 493.1013\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 9s 841ms/step - loss: 450.3665\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 9s 809ms/step - loss: 411.2981\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 9s 842ms/step - loss: 374.9477\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 9s 817ms/step - loss: 342.0908\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 9s 826ms/step - loss: 311.9326\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 9s 822ms/step - loss: 284.5271\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 9s 833ms/step - loss: 259.5408\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 10s 879ms/step - loss: 236.8553\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 9s 835ms/step - loss: 216.1136\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 9s 827ms/step - loss: 197.4390\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 9s 824ms/step - loss: 180.5931\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 9s 826ms/step - loss: 165.1406\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 9s 828ms/step - loss: 151.5664\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 9s 832ms/step - loss: 139.0519\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 9s 840ms/step - loss: 127.9594\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 9s 836ms/step - loss: 118.1727\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 9s 814ms/step - loss: 109.2531\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 9s 833ms/step - loss: 101.4268\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 10s 877ms/step - loss: 94.2726\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 9s 847ms/step - loss: 88.1133\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 9s 802ms/step - loss: 82.5430\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 9s 832ms/step - loss: 77.7068\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 9s 823ms/step - loss: 73.3953\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 10s 877ms/step - loss: 69.6223\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 9s 795ms/step - loss: 66.3487\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 9s 841ms/step - loss: 63.3618\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 9s 791ms/step - loss: 60.8467\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 9s 836ms/step - loss: 58.7094\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 9s 804ms/step - loss: 56.7255\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 9s 833ms/step - loss: 55.0380\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 9s 813ms/step - loss: 53.5059\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 9s 847ms/step - loss: 52.3508\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 9s 819ms/step - loss: 51.1895\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 9s 849ms/step - loss: 50.2957\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 9s 842ms/step - loss: 49.3918\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 9s 837ms/step - loss: 48.7710\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 9s 817ms/step - loss: 48.1769\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 9s 859ms/step - loss: 47.6580\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 9s 844ms/step - loss: 47.2031\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 10s 928ms/step - loss: 46.8384\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 9s 824ms/step - loss: 46.5527\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 9s 843ms/step - loss: 46.2379\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 9s 815ms/step - loss: 46.0278\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 9s 845ms/step - loss: 45.8412\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 9s 850ms/step - loss: 45.6468\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 9s 859ms/step - loss: 45.5459\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 9s 805ms/step - loss: 45.4110\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 9s 846ms/step - loss: 45.3454\n",
      "Epoch 66/200\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 32.5130"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-17e32cf1a20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input = np.array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, 3, 2))\n",
    "x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        y\n",
       "0  159.80\n",
       "1  158.41\n",
       "2  157.49\n",
       "3  156.11\n",
       "4  154.28"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0:5, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(dataset.iloc[17487:18718, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(dataset.iloc[17487:18718, 3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[0:n_steps_in].reshape((1, n_steps_in, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(x_test, verbose=0)\n",
    "len(yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[n_steps_in:n_steps_in+n_steps_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc16c1e02e0>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(yhat[0])\n",
    "plt.plot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
